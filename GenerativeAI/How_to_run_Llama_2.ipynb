{"cells":[{"cell_type":"markdown","metadata":{"id":"4bKQIsIq-d8y"},"source":["## **Llama 2**\n","\n","- The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases.\n","- It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety.\n","- [Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)\n","- `llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n","\n","- `GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage.\n","\n","## Quantized Models:\n","\n","- Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n","\n","- Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.\n","\n","#### Quantized Models from the Hugging Face Community\n","\n","- The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n","\n","- There are several variations available, but the ones that interest us are based on the GGLM library.\n","\n","- We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n","\n","- In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML).\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702896492324,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"v_dliwERb6Uh","outputId":"f83fa79b-e26a-4daf-fee3-c0e47d91b9a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Dec 18 10:48:12 2023       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128919,"status":"ok","timestamp":1702896628340,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"L0avf7xx2lcj","outputId":"2e710f37-5e86-43bb-95bd-8bd3b35c5419"},"outputs":[],"source":["# # GPU llama-cpp-python\n","# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n","# !pip install huggingface_hub\n","# !pip install llama-cpp-python==0.1.78\n","# !pip install numpy==1.23.4"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1702896632386,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"qJ90LnMv54Y-"},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1702896724416,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"kL3LF6pfE3HD","outputId":"92e1bbf2-4017-4eab-f4d8-ea7b5afa57f6"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import hf_hub_download\n","from llama_cpp import Llama\n","\n","# Download the model from the Hugging Face Hub\n","model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n","model_path"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35383,"status":"ok","timestamp":1702896762025,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"irftToUj6aWt","outputId":"61b67c86-fdc4-4c2c-9abe-239f9fa682e8"},"outputs":[{"name":"stderr","output_type":"stream","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"]}],"source":["# Load the model\n","\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1702896766730,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"YG4Pylz662At","outputId":"9f5adc1a-3181-412c-de9c-a268ce99f1a0"},"outputs":[{"data":{"text/plain":["32"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# See the number of layers in GPU\n","lcpp_llm.params.n_gpu_layers"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1702896773148,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"RfzwELMC7Dyg"},"outputs":[],"source":["# Create a prompt\n","prompt = \"Write a linear regression code\"\n","\n","prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n","\n","USER: {prompt}\n","\n","ASSISTANT:\n","'''"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1702896993349,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"jlJ1JgR68DDO","outputId":"e678959c-96fe-4e46-b101-edf5f85a5139"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'id': 'cmpl-b113236e-295b-488b-b0a0-8e6a770f7d69', 'object': 'text_completion', 'created': 1702896876, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': \"SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Write a linear regression code\\n\\nASSISTANT:\\n\\nTo write a linear regression code, you can use the scikit-learn library in Python. Here is an example of how to do this:\\n```\\nimport pandas as pd\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load your dataset\\ndf = pd.read_csv('your_data.csv')\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2)\\n\\n# Create a Linear Regression object and fit it to the data\\nlr_model = LinearRegression()\\nlr_model.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = lr_model.predict(X_test)\\n\\n# Evaluate the model using mean squared error (MSE)\\nmse = ((y_test - y_pred) ** 2).mean()\\nprint('Mean Squared Error:', mse)\\n```\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 256, 'total_tokens': 294}}\n"]}],"source":["# Generate a response\n","response = lcpp_llm(prompt=prompt_template, \n","                    max_tokens=256, # The maximum number of tokens to generate\n","                    temperature=0.5, # The value used to module the next token probabilities\n","                    top_p=0.95, # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n","                    repeat_penalty=1.2,  # The parameter for repetition penalty. 1.0 means no penalty.\n","                    top_k=150, # The number of highest probability vocabulary tokens to keep for top-k-filtering.\n","                    echo=True )\n","print(response)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702896995188,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"Qona58gX8oAn","outputId":"3e977582-ccde-4ef6-d8ce-a3616d604faf"},"outputs":[{"name":"stdout","output_type":"stream","text":["SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n","\n","USER: Write a linear regression code\n","\n","ASSISTANT:\n","\n","To write a linear regression code, you can use the scikit-learn library in Python. Here is an example of how to do this:\n","```\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","# Load your dataset\n","df = pd.read_csv('your_data.csv')\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2)\n","\n","# Create a Linear Regression object and fit it to the data\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","\n","# Make predictions on the testing set\n","y_pred = lr_model.predict(X_test)\n","\n","# Evaluate the model using mean squared error (MSE)\n","mse = ((y_test - y_pred) ** 2).mean()\n","print('Mean Squared Error:', mse)\n","```\n"]}],"source":["print(response[\"choices\"][0][\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4uMV0zF8pQt"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c1df7d59afa4b158bada2516c2c93fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"124cfafd45044f6dae7d237449cab2b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"167854af5ba345c887b946413dc1eba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_532e060cd7a541f68db6a6e7b30eb1ea","placeholder":"​","style":"IPY_MODEL_3b81fc9c8de84eff8b2083adfa7c9484","value":"llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"}},"1920085e1ee740edbad6ef207fec327f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62981a143891468db7b7d8df19f9fb6d","placeholder":"​","style":"IPY_MODEL_124cfafd45044f6dae7d237449cab2b4","value":" 9.76G/9.76G [01:23&lt;00:00, 107MB/s]"}},"39a396c8527c49dc946b6ed6ff851d6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c1df7d59afa4b158bada2516c2c93fe","max":9763701888,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78c35049b2494b7283d4e3841225aaee","value":9763701888}},"3b81fc9c8de84eff8b2083adfa7c9484":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"532e060cd7a541f68db6a6e7b30eb1ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61f728f470a64d7ba96c1ea7f534ea2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_167854af5ba345c887b946413dc1eba6","IPY_MODEL_39a396c8527c49dc946b6ed6ff851d6b","IPY_MODEL_1920085e1ee740edbad6ef207fec327f"],"layout":"IPY_MODEL_ef7d2457fa03495198c01f461b61ae28"}},"62981a143891468db7b7d8df19f9fb6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78c35049b2494b7283d4e3841225aaee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef7d2457fa03495198c01f461b61ae28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
