{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xa4CUDIuEYye"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "# Create this kaggle.json from kaggle profile->Account->Create new API token\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gc5FIAAEd9l",
        "outputId": "8efa3a43-ab81-4e5e-a74b-8f52a1392eea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 1.06G/1.06G [00:50<00:00, 23.5MB/s]\n",
            "100% 1.06G/1.06G [00:50<00:00, 22.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/dogs-vs-cats.zip\",'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Fx-S8Mc3EhgI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout"
      ],
      "metadata": {
        "id": "e0EVSjFPH7EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generators: It is used to work with large data set.\n",
        "# It will load images in the RAM as needed\n",
        "\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/train',\n",
        "    labels='inferred',\n",
        "    label_mode=\"int\", # It will assign cat 0 and Dog 1\n",
        "    batch_size=32,\n",
        "    image_size=(256, 256), # Every image having diffrent size, it will reshape to 256*256\n",
        ")\n",
        "\n",
        "\n",
        "test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/test',\n",
        "    labels='inferred',\n",
        "    label_mode=\"int\", # It will assign cat 0 and Dog 1\n",
        "    batch_size=32,\n",
        "    image_size=(256, 256), # Every image having diffrent size, it will reshape to 256*256\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExRI4j8QIdXY",
        "outputId": "f238b498-5fcb-4148-a365-0bb64664d7a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here, image pixels is stored as numpy array range from 0 to 255\n",
        "# So converting it into 0-1\n",
        "# Normalizing  it\n",
        "\n",
        "def process(image, label):\n",
        "  image = tf.cast(image/255. ,tf.float32)\n",
        "  return image, label\n",
        "\n",
        "train_ds = train_ds.map(process)\n",
        "validation_ds = test_ds.map(process)"
      ],
      "metadata": {
        "id": "XG0vKZrAJxJc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Normal CNN model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu', input_shape=(256,256,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3,3), padding='valid', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3,3), padding='valid', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "52BBbavGK2EG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Q9d1OwYUCD",
        "outputId": "5335fc7f-411e-4057-96c7-5b7ec8b429af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 115200)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               14745728  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14847297 (56.64 MB)\n",
            "Trainable params: 14847297 (56.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lRNb9oAlYZlp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory = model.fit(train_ds, epochs=10, validation_data=test_ds)"
      ],
      "metadata": {
        "id": "6bNsUlbJYsxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], color='red', label='train')\n",
        "plt.plot(history.history[\"val_accuracy\"], color='blue', label='Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ug0iTuHY8jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], color='red', label='train')\n",
        "plt.plot(history.history[\"val_loss\"], color='blue', label='Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FxJdOKIza4vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above graph, can say that model is overfitted"
      ],
      "metadata": {
        "id": "ffDasbBQbEgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Diffrent ways to reduce overfittings\n",
        "\n",
        "- Add more data\n",
        "- Data Augmentation\n",
        "- L1/L2 regularizer\n",
        "- Dropout\n",
        "- Batch Norm\n",
        "- Reduce Complexity"
      ],
      "metadata": {
        "id": "_1941DvQbSX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Batch Normalizer and dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu', input_shape=(256,256,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3,3), padding='valid', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3,3), padding='valid', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "XdnYc5rCa7nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fFG6sgvVcnVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostory = model.fit(train_ds, epochs=10, validation_data=test_ds)"
      ],
      "metadata": {
        "id": "ffigqnZvcwwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], color='red', label='train')\n",
        "plt.plot(history.history[\"val_accuracy\"], color='blue', label='Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CiBA81qDcyt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], color='red', label='train')\n",
        "plt.plot(history.history[\"val_loss\"], color='blue', label='Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XEu6XkUpc19Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Performance increases a bit, still we can improve it using different methods"
      ],
      "metadata": {
        "id": "bLBS7ctAdCKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "BpfohXgxdL0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "Ha3S_yHTdKuy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = cv2.imread(\"/content/dog.jpg\")"
      ],
      "metadata": {
        "id": "qWeOVBqOdPT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img)"
      ],
      "metadata": {
        "id": "GXkh1FjHdnlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img.shape"
      ],
      "metadata": {
        "id": "ZXfV1rlfdswm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing test image\n",
        "test_img = cv2.resize(test_img, (256,256))\n",
        "\n",
        "# Again resizing it, since we are giving model in batches (1, 256,256, 3): (<1st Batch>, (256,256, (RGB sp 3 layers))\n",
        "test_input = test_img.reshape((1, 256, 256, 3))"
      ],
      "metadata": {
        "id": "trIcMXSed0qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(test_input)"
      ],
      "metadata": {
        "id": "AfIxqX58hNRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}