{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "In NLP, word embedding is a term used for the represention of word for text analysis, typically in the form of a real-valued that encodes the meaning of the word such that the words that are closer in the vector space are exprected o be similar in meaning\n",
    "\n",
    "Types:\n",
    "\n",
    "1. Frequency based (Matrix Formation):\n",
    "\n",
    "- BOW\n",
    "- TFIDF\n",
    "- Glove\n",
    "\n",
    "2. Prediction Based (Deep Learning algo):\n",
    "\n",
    "- Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mejor advantage\n",
    "\n",
    "- In the Word2vec we can get symantic meaning of the words\n",
    "- Low dimenions conmpare to matrix methos\n",
    "- Dence Vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of model used:\n",
    "\n",
    "1. Pretrained model on google 3 Billion news crpus\n",
    "2. Custom trained model with a small dataset (around 50k sentences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google trained model\n",
    "#\n",
    "\n",
    "# # !pip install gensim\n",
    "# !pip install wget\n",
    "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec, keyedVectors\n",
    "\n",
    "# model =  keyedVectors.load_word2vec_format.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# model[\"man\"].shape\n",
    "# model[\"phone\"].shape\n",
    "# model[\"cricket\"].shape\n",
    "\n",
    "# model.most_similar(\"man\")\n",
    "# model.most_similar(\"cricket\")\n",
    "# model.most_similar(\"facebook\")\n",
    "\n",
    "# model.similarity(\"man\", \"woman\")\n",
    "# model.similarity(\"man\", \"JavaScript\")\n",
    "\n",
    "# model.doesnt_match(\"JAVA\", \"CPP\", \"man\")\n",
    "\n",
    "# vec = model[\"king\"] - model[\"man\"] + model[\"woman\"]\n",
    "# model.most_similar([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model.\n",
    "\n",
    "#### Typs of Word2Vec\n",
    "\n",
    "- Skipgram (SG) : 1 hidden layer, input word and output context words.\n",
    "- CBOW(Continuous Bag Of Words): 1 hidden layer, input is a window of surrounding words to predict the center word in that\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
