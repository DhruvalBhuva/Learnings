{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corpus (C): Combination of all words of the given dataset. Words can be repeatative\n",
    "- Vocabulary (V): Unique words of Corpus.\n",
    "- Document (D): Each records is called as Documents.\n",
    "- Word (W): Each word of document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot Encoding\n",
    "\n",
    "- Pros: Easy to intitive and implement\n",
    "- Cons: Sparcity (it also leads to overfitting), Different dimensionality for each document, need to tain again while changing vocab other than traing vacab, No capturing of semantic meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "- Mostly using in Text classification and its effective\n",
    "- It classify text according to it's frequency of the word, order and sentence of the word doesn't matter\n",
    "- Pros: Simple and intuitive\n",
    "- Cons: Sensitive to outliers(words that are rare),Sparcity, handling Out of vocab word (As it ignores new words), Order is ignored of the words, fails to understand semantic meaning. i.e: \"This is a good car\" and \"This is not a good car\". since both the sentence having almost same frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch man</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man watch people</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people write people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  output\n",
       "0      people watch man       1\n",
       "1  people write comment       1\n",
       "2      man watch people       0\n",
       "3   people write people       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"text\": [\"people watch man\", \"people write comment\",\n",
    "                  \"man watch people\", \"people write people\"], \"output\": [1, 1, 0, 1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'man': 1, 'write': 4, 'comment': 0}\n"
     ]
    }
   ],
   "source": [
    "# Vocab\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0]]\n",
      "[[1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# print(bow[0])\n",
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"Man watch and write comment of people Man\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-Grams(Uni-Grams/Bi-Grams/Tri-Grams....)\n",
    "\n",
    "- Uni-Gram : Unigram is a single word in the sentence, it can be considered as one feature.\n",
    "- Bi-gram: It considers two words at once to predict next word.\n",
    "- Tri-grams: It considers three words at once and so on...\n",
    "- when we set ngram_range param as (1,1) it;s calels bag of words/ Uni gram. as same as for bi-gram value would be (2,2)\n",
    "- Prons: Able to identify sementic meaning.\n",
    "- Cons: As N values get increases it would be more time consuming, Out of vocab problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "cv.fit_transform(df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch': 1,\n",
       " 'watch man': 3,\n",
       " 'people write': 2,\n",
       " 'write comment': 5,\n",
       " 'man watch': 0,\n",
       " 'watch people': 4,\n",
       " 'write people': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 3,\n",
       " 'watch': 6,\n",
       " 'man': 1,\n",
       " 'people watch': 4,\n",
       " 'watch man': 7,\n",
       " 'write': 9,\n",
       " 'comment': 0,\n",
       " 'people write': 5,\n",
       " 'write comment': 10,\n",
       " 'man watch': 2,\n",
       " 'watch people': 8,\n",
       " 'write people': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we set ngram_range=(1,2), it would be combination of Uni and Bi gram combined\n",
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "cv.fit_transform(df[\"text\"]).toarray()\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IF: erm Frequency\n",
    "- IDF: Inverse Dot frequency\n",
    "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "- IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "- TF-IDF = TF \\* IDF\n",
    "- In the IF-IDF we assign a weightage to each words using TF and IDF. That words having heighest weitage though it is not frequently used in Corpus, but it is in perticuler document, so for that document that word would have heighest weightage.\n",
    "- IF also represent probability of that word in perticuler document.\n",
    "- IDF is logarithmically proportional to inverse number of documents containing this word, which means it will be small when there are many docs. IDF is logarithmically scaled down to prevent extreme values from dominating results. The higher the value, the more important it is for search engine to find this particular word in documents.\n",
    "- If we have many words and they are not common then their idf will be very small which means these words are less significant than other words.\n",
    "- If we have many words with high tf and low idf then they are considered as stopwords which will not be useful for searching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interview Question: Why log is taken while calculating IDF ?\n",
    "\n",
    "- Suppose there is a word, which having only one use in entire corpus of 10k document. So with our log IDF would be 10k it's big num. and after multiplying it with ID, IDF's weightage would be more. so log is taken. with the log that value would be 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.64043405, 0.42389674, 0.64043405, 0.        ],\n",
       "       [0.72664149, 0.        , 0.37919167, 0.        , 0.5728925 ],\n",
       "       [0.        , 0.64043405, 0.42389674, 0.64043405, 0.        ],\n",
       "       [0.        , 0.        , 0.7979221 , 0.        , 0.60276058]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.51082562 1.         1.51082562 1.51082562]\n",
      "['comment' 'man' 'people' 'watch' 'write']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prons: Use full for Information Retirival\n",
    "- Cons: Sparsity, out of vocab Problem for prediction, dimention, Symentic relation not captured\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While creating a project we required custom features.\n",
    "- For an example sentimec analysis project features could be\n",
    "- Number of +ve word\n",
    "- Number of -ve word\n",
    "- Ratio of +ve/-ve words\n",
    "- Char count\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
